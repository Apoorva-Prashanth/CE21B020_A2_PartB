{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Loading the data set"
      ],
      "metadata": {
        "id": "7_m_DgO1M8Ys"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FJ09hF09LAG",
        "outputId": "7d032a24-4909-4231-ffb0-ba9a58738bd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Data prepared with 80% training and 20% validation split. The test data remains in: /content/inaturalist_12K/test\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import zipfile\n",
        "import os\n",
        "import shutil\n",
        "from torchvision.datasets import ImageFolder\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to the ZIP file in your Google Drive\n",
        "zip_path = '/content/drive/MyDrive/nature_12K.zip'  # Adjust this based on the actual path in your Drive\n",
        "\n",
        "# Extract the ZIP file\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/')  # Extract to /content/\n",
        "\n",
        "# Original and new paths\n",
        "extracted_folder = '/content/inaturalist_12K'\n",
        "original_val_path = os.path.join(extracted_folder, 'val')\n",
        "new_test_path = os.path.join(extracted_folder, 'test')\n",
        "\n",
        "# Rename 'val' to 'test'\n",
        "if os.path.exists(original_val_path):\n",
        "    os.rename(original_val_path, new_test_path)\n",
        "\n",
        "# Define the paths for the training data\n",
        "original_train_dir = os.path.join(extracted_folder, 'train')\n",
        "new_train_dir = os.path.join(extracted_folder, 'train_split')  # Keep it same\n",
        "new_val_dir = os.path.join(extracted_folder, 'val_split')  # This will be used for new validation data\n",
        "\n",
        "# Load the samples and labels using ImageFolder\n",
        "dataset = ImageFolder(root=original_train_dir)\n",
        "samples = dataset.samples  # List of (path, class_index)\n",
        "labels = [label for _, label in samples]\n",
        "\n",
        "# Perform a stratified split (80% train, 20% validation)\n",
        "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "train_idx, val_idx = next(splitter.split(samples, labels))\n",
        "\n",
        "train_samples = [samples[i] for i in train_idx]\n",
        "val_samples = [samples[i] for i in val_idx]\n",
        "\n",
        "# Create new folders for the validation set and training set\n",
        "for class_name in dataset.classes:\n",
        "    os.makedirs(os.path.join(new_train_dir, class_name), exist_ok=True)\n",
        "    os.makedirs(os.path.join(new_val_dir, class_name), exist_ok=True)\n",
        "\n",
        "# Copy files to the new training folders\n",
        "for path, label in train_samples:\n",
        "    class_name = dataset.classes[label]\n",
        "    dest = os.path.join(new_train_dir, class_name, os.path.basename(path))\n",
        "    shutil.copy2(path, dest)\n",
        "\n",
        "# Copy files to the new validation folders\n",
        "for path, label in val_samples:\n",
        "    class_name = dataset.classes[label]\n",
        "    dest = os.path.join(new_val_dir, class_name, os.path.basename(path))\n",
        "    shutil.copy2(path, dest)\n",
        "\n",
        "print(f\"Data prepared with 80% training and 20% validation split. The test data remains in: {new_test_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trial 1: freeze all layers except the final fully connected layer"
      ],
      "metadata": {
        "id": "uVTONS7Perno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# ImageNet standard transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "train_dataset = datasets.ImageFolder(root=new_train_dir, transform=transform)\n",
        "val_dataset = datasets.ImageFolder(root=new_val_dir, transform=transform)\n",
        "test_dataset = datasets.ImageFolder(root=new_test_path, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
      ],
      "metadata": {
        "id": "HUX-iZVIBVzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load pre-trained ResNet50\n",
        "model = models.resnet50(pretrained=True)\n",
        "\n",
        "# Freeze all layers\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Replace final layer\n",
        "num_features = model.fc.in_features\n",
        "model.fc = nn.Linear(num_features, 10)  # 10 iNaturalist classes\n",
        "model = model.to(device)\n"
      ],
      "metadata": {
        "id": "rMVZWocNBaLy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7effc0da-892a-405c-c9e9-a2535930b23d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 161MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.fc.parameters(), lr=1e-2)  # Only train final layer\n"
      ],
      "metadata": {
        "id": "-Wva71a5BcXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, dataloader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    running_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return running_loss / total, correct / total\n",
        "\n",
        "# Training loop\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    train_loss = running_loss / total\n",
        "    train_acc = correct / total\n",
        "    val_loss, val_acc = evaluate(model, val_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} --> \"\n",
        "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
        "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "v75FdHbeBgbH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "973114f3-4da7-4db7-8cf6-cfaf044fc1dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 --> Train Loss: 0.9050, Train Acc: 0.7810, Val Loss: 1.0967, Val Acc: 0.7590\n",
            "Epoch 2/10 --> Train Loss: 0.8579, Train Acc: 0.7817, Val Loss: 1.0631, Val Acc: 0.7640\n",
            "Epoch 3/10 --> Train Loss: 0.8425, Train Acc: 0.7855, Val Loss: 1.0498, Val Acc: 0.7620\n",
            "Epoch 4/10 --> Train Loss: 0.7997, Train Acc: 0.7906, Val Loss: 1.0274, Val Acc: 0.7665\n",
            "Epoch 5/10 --> Train Loss: 0.8040, Train Acc: 0.7885, Val Loss: 1.0338, Val Acc: 0.7630\n",
            "Epoch 6/10 --> Train Loss: 0.7705, Train Acc: 0.7932, Val Loss: 1.0273, Val Acc: 0.7645\n",
            "Epoch 7/10 --> Train Loss: 0.7533, Train Acc: 0.7956, Val Loss: 1.0080, Val Acc: 0.7640\n",
            "Epoch 8/10 --> Train Loss: 0.7507, Train Acc: 0.7930, Val Loss: 1.0120, Val Acc: 0.7660\n",
            "Epoch 9/10 --> Train Loss: 0.7313, Train Acc: 0.7981, Val Loss: 1.0140, Val Acc: 0.7640\n",
            "Epoch 10/10 --> Train Loss: 0.7281, Train Acc: 0.7976, Val Loss: 0.9869, Val Acc: 0.7670\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = evaluate(model, test_loader)\n",
        "print(f\"\\nTest Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "RdmarlB1BkLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results are much better than training from scratch. However, let us see if we can improve the performance futher."
      ],
      "metadata": {
        "id": "1gr4Ka9gPYwS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the below code, changing the value of k changes the last k number of layers to unfreeze. I have chnaged it to k=1 and k=2. However k=2 led to overfitting. But validation accuracy was more than k=1. So to prevent overfitting I added data augmentation for k=2."
      ],
      "metadata": {
        "id": "v7UkuKE-Prtd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import os\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Paths\n",
        "data_root = '/content/inaturalist_12K'\n",
        "train_dir = os.path.join(data_root, 'train_split')\n",
        "val_dir   = os.path.join(data_root, 'val_split')\n",
        "test_dir  = os.path.join(data_root, 'test')\n",
        "\n",
        "# Transforms\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
        "])\n",
        "val_test_transform = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "train_ds = datasets.ImageFolder(train_dir, transform=train_transform)\n",
        "val_ds   = datasets.ImageFolder(val_dir,   transform=val_test_transform)\n",
        "test_ds  = datasets.ImageFolder(test_dir,  transform=val_test_transform)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True,  num_workers=2)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=32, shuffle=False, num_workers=2)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "# Model\n",
        "model = models.resnet50(pretrained=True)\n",
        "# add dropout before final layer\n",
        "in_feats = model.fc.in_features\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Dropout(p=0.5),\n",
        "    nn.Linear(in_feats, 10)\n",
        ")\n",
        "model = model.to(device)\n",
        "\n",
        "# Freeze all but last k layers\n",
        "def unfreeze_last_k_layers(model, k=1):\n",
        "    for p in model.parameters(): p.requires_grad = False\n",
        "    for p in model.fc.parameters(): p.requires_grad = True\n",
        "    layers = [model.layer4, model.layer3, model.layer2, model.layer1]\n",
        "    for i in range(k):\n",
        "        for p in layers[i].parameters(): p.requires_grad = True\n",
        "\n",
        "unfreeze_last_k_layers(model, k=2)\n",
        "\n",
        "# Optimizer, loss, scheduler\n",
        "optimizer = optim.AdamW(\n",
        "    filter(lambda p: p.requires_grad, model.parameters()),\n",
        "    lr=1e-4,\n",
        "    weight_decay=1e-2\n",
        ")\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, verbose=True)\n",
        "\n",
        "# Eval helper\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    total_c, total_l = 0, 0.0\n",
        "    with torch.no_grad():\n",
        "        for x,y in loader:\n",
        "            x,y = x.to(device), y.to(device)\n",
        "            out = model(x)\n",
        "            loss = criterion(out,y)\n",
        "            total_l += loss.item()\n",
        "            preds = out.argmax(dim=1)\n",
        "            total_c += (preds==y).sum().item()\n",
        "    return total_c/len(loader.dataset), total_l/len(loader)\n",
        "\n",
        "# Training with Early Stopping\n",
        "def train_model(model, train_loader, val_loader, epochs=20, patience=4):\n",
        "    best_acc, counter = 0.0, 0\n",
        "    best_wts = model.state_dict()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss, running_correct = 0.0, 0\n",
        "\n",
        "        for x,y in train_loader:\n",
        "            x,y = x.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            out = model(x)\n",
        "            loss = criterion(out, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            running_correct += (out.argmax(1)==y).sum().item()\n",
        "\n",
        "        train_acc = running_correct/len(train_loader.dataset)\n",
        "        val_acc, val_loss = evaluate(model, val_loader)\n",
        "        print(f\"Epoch {epoch+1}: Train Acc {train_acc*100:.2f}%, Val Acc {val_acc*100:.2f}%\")\n",
        "\n",
        "        # LR scheduling on val accuracy\n",
        "        scheduler.step(val_acc)\n",
        "\n",
        "        # Early stopping tracking\n",
        "        if val_acc > best_acc:\n",
        "            best_acc, counter = val_acc, 0\n",
        "            best_wts = model.state_dict()\n",
        "        else:\n",
        "            counter += 1\n",
        "            if counter >= patience:\n",
        "                print(\"Early stopping at epoch\", epoch+1)\n",
        "                break\n",
        "\n",
        "    model.load_state_dict(best_wts)\n",
        "\n",
        "# Run\n",
        "train_model(model, train_loader, val_loader, epochs=20, patience=4)\n",
        "test_acc, test_loss = evaluate(model, test_loader)\n",
        "print(f\"Test Acc: {test_acc*100:.2f}% | Test Loss: {test_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "TEtN8nESC4L_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba62faf6-ea13-4063-8879-b2532d5098f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 177MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Acc 65.35%, Val Acc 77.15%\n",
            "Epoch 2: Train Acc 79.26%, Val Acc 78.20%\n",
            "Epoch 3: Train Acc 84.65%, Val Acc 79.45%\n",
            "Epoch 4: Train Acc 88.50%, Val Acc 79.40%\n",
            "Epoch 5: Train Acc 90.64%, Val Acc 78.40%\n",
            "Epoch 6: Train Acc 92.81%, Val Acc 77.80%\n",
            "Epoch 7: Train Acc 96.10%, Val Acc 81.60%\n",
            "Epoch 8: Train Acc 97.52%, Val Acc 81.00%\n",
            "Epoch 9: Train Acc 98.30%, Val Acc 82.45%\n",
            "Epoch 10: Train Acc 98.39%, Val Acc 81.25%\n",
            "Epoch 11: Train Acc 98.55%, Val Acc 81.85%\n",
            "Epoch 12: Train Acc 98.65%, Val Acc 81.20%\n",
            "Epoch 13: Train Acc 99.34%, Val Acc 82.40%\n",
            "Early stopping at epoch 13\n",
            "Test Acc: 81.75% | Test Loss: 0.9545\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, Trying strategy 3: k=2, with data augmentation and logging the results in W&B"
      ],
      "metadata": {
        "id": "dSWwyNJzUDd8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import os\n",
        "import wandb\n",
        "\n",
        "# Init Weights & Biases\n",
        "wandb.init(project=\"iNaturalist_CNN_da6401_A2\", name=\"resnet50_finetune_k2\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Paths\n",
        "data_root = '/content/inaturalist_12K'\n",
        "train_dir = os.path.join(data_root, 'train_split')\n",
        "val_dir   = os.path.join(data_root, 'val_split')\n",
        "test_dir  = os.path.join(data_root, 'test')\n",
        "\n",
        "# Transforms\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
        "])\n",
        "val_test_transform = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "train_ds = datasets.ImageFolder(train_dir, transform=train_transform)\n",
        "val_ds   = datasets.ImageFolder(val_dir,   transform=val_test_transform)\n",
        "test_ds  = datasets.ImageFolder(test_dir,  transform=val_test_transform)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True,  num_workers=2)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=32, shuffle=False, num_workers=2)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "# Model setup\n",
        "model = models.resnet50(pretrained=True)\n",
        "in_feats = model.fc.in_features\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Dropout(p=0.5),\n",
        "    nn.Linear(in_feats, 10)\n",
        ")\n",
        "model = model.to(device)\n",
        "\n",
        "# Unfreeze last k blocks\n",
        "def unfreeze_last_k_layers(model, k=2):\n",
        "    for p in model.parameters(): p.requires_grad = False\n",
        "    for p in model.fc.parameters(): p.requires_grad = True\n",
        "    layers = [model.layer4, model.layer3, model.layer2, model.layer1]\n",
        "    for i in range(k):\n",
        "        for p in layers[i].parameters(): p.requires_grad = True\n",
        "\n",
        "unfreeze_last_k_layers(model, k=2)\n",
        "\n",
        "# Optimizer, loss, scheduler\n",
        "optimizer = optim.AdamW(\n",
        "    filter(lambda p: p.requires_grad, model.parameters()),\n",
        "    lr=1e-4,\n",
        "    weight_decay=1e-2\n",
        ")\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, verbose=True)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    total_correct, total_loss = 0, 0.0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            out = model(x)\n",
        "            loss = criterion(out, y)\n",
        "            total_loss += loss.item()\n",
        "            total_correct += (out.argmax(dim=1) == y).sum().item()\n",
        "    acc = total_correct / len(loader.dataset)\n",
        "    avg_loss = total_loss / len(loader)\n",
        "    return acc, avg_loss\n",
        "\n",
        "# Training loop with early stopping and wandb logging\n",
        "def train_model(model, train_loader, val_loader, epochs=20, patience=4):\n",
        "    best_acc, counter = 0.0, 0\n",
        "    best_weights = model.state_dict()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss, running_correct = 0.0, 0\n",
        "\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            out = model(x)\n",
        "            loss = criterion(out, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            running_correct += (out.argmax(1) == y).sum().item()\n",
        "\n",
        "        train_acc = running_correct / len(train_loader.dataset)\n",
        "        train_loss = running_loss / len(train_loader)\n",
        "        val_acc, val_loss = evaluate(model, val_loader)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: Train Acc {train_acc*100:.2f}%, Val Acc {val_acc*100:.2f}%\")\n",
        "\n",
        "        # wandb logging\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"train_acc\": train_acc,\n",
        "            \"train_loss\": train_loss,\n",
        "            \"val_acc\": val_acc,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"lr\": optimizer.param_groups[0][\"lr\"]\n",
        "        })\n",
        "\n",
        "        scheduler.step(val_acc)\n",
        "\n",
        "        # Early stopping\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            best_weights = model.state_dict()\n",
        "            counter = 0\n",
        "        else:\n",
        "            counter += 1\n",
        "            if counter >= patience:\n",
        "                print(\"Early stopping triggered at epoch\", epoch+1)\n",
        "                break\n",
        "\n",
        "    model.load_state_dict(best_weights)\n",
        "\n",
        "#Train\n",
        "train_model(model, train_loader, val_loader, epochs=20, patience=4)\n",
        "\n",
        "#Test\n",
        "test_acc, test_loss = evaluate(model, test_loader)\n",
        "print(f\"Test Accuracy: {test_acc*100:.2f}%, Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "#log final test metrics to wandb\n",
        "wandb.log({\n",
        "    \"test_acc\": test_acc,\n",
        "    \"test_loss\": test_loss\n",
        "})\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "id": "MQiEbAUJjOxh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c9ccf4f9-45db-4043-f1e6-21aaf05b06d2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mapoorvaprashanth\u001b[0m (\u001b[33mapoorvaprashanth-indian-institute-of-technology-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250419_182142-hx1q76sq</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/iNaturalist_CNN_da6401_A2/runs/hx1q76sq' target=\"_blank\">resnet50_finetune_k2</a></strong> to <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/iNaturalist_CNN_da6401_A2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/iNaturalist_CNN_da6401_A2' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/iNaturalist_CNN_da6401_A2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/iNaturalist_CNN_da6401_A2/runs/hx1q76sq' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/iNaturalist_CNN_da6401_A2/runs/hx1q76sq</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 150MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Acc 65.93%, Val Acc 77.10%\n",
            "Epoch 2: Train Acc 79.27%, Val Acc 78.25%\n",
            "Epoch 3: Train Acc 84.77%, Val Acc 78.45%\n",
            "Epoch 4: Train Acc 88.32%, Val Acc 77.95%\n",
            "Epoch 5: Train Acc 90.47%, Val Acc 77.30%\n",
            "Epoch 6: Train Acc 92.55%, Val Acc 78.35%\n",
            "Epoch 7: Train Acc 95.99%, Val Acc 81.75%\n",
            "Epoch 8: Train Acc 97.32%, Val Acc 81.95%\n",
            "Epoch 9: Train Acc 98.22%, Val Acc 81.40%\n",
            "Epoch 10: Train Acc 98.41%, Val Acc 81.55%\n",
            "Epoch 11: Train Acc 98.69%, Val Acc 82.25%\n",
            "Epoch 12: Train Acc 98.80%, Val Acc 81.75%\n",
            "Epoch 13: Train Acc 98.99%, Val Acc 80.90%\n",
            "Epoch 14: Train Acc 98.75%, Val Acc 82.15%\n",
            "Epoch 15: Train Acc 99.40%, Val Acc 81.95%\n",
            "Early stopping triggered at epoch 15\n",
            "Test Accuracy: 82.35%, Test Loss: 0.9587\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>lr</td><td>██████▃▃▃▃▃▃▃▃▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▄▅▆▆▇▇████████</td></tr><tr><td>train_loss</td><td>█▅▄▃▃▃▂▂▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▃▃▂▁▃▇█▇▇█▇▆██</td></tr><tr><td>val_loss</td><td>▅▆▆▆██▂▂▂▂▁▂▃▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>lr</td><td>3e-05</td></tr><tr><td>test_acc</td><td>0.8235</td></tr><tr><td>test_loss</td><td>0.95865</td></tr><tr><td>train_acc</td><td>0.994</td></tr><tr><td>train_loss</td><td>0.54647</td></tr><tr><td>val_acc</td><td>0.8195</td></tr><tr><td>val_loss</td><td>0.97015</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">resnet50_finetune_k2</strong> at: <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/iNaturalist_CNN_da6401_A2/runs/hx1q76sq' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/iNaturalist_CNN_da6401_A2/runs/hx1q76sq</a><br> View project at: <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/iNaturalist_CNN_da6401_A2' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/iNaturalist_CNN_da6401_A2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250419_182142-hx1q76sq/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qnkBp1OaR-sK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}